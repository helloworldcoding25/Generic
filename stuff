"""
Skew helper for corporate-bond RFQs.

Inputs
------
idio_pred_bp : float
    Model-predicted *EOD idiosyncratic* spread move in bp.
    > 0  → expected widening
    < 0  → expected tightening
side        : "buy" | "sell"
    From the client’s perspective (client buys → dealer sells).
mid_spread  : float
    Mid-market spread of the bond (bp).
cfg         : SkewConfig
    Tuning parameters (linear slope, asymmetry tables, hard cap).

Returns
-------
quoted_spread : float
    Mid spread + skew (bp).
skew_bp       : float
    The skew that was applied (bp) – useful for logging / ML features.
"""
from dataclasses import dataclass, field
from math import copysign, tanh

@dataclass
class SkewConfig:
    # --- core scale settings -------------------------------------------------
    base_slope: float = 0.6        # bp of skew per 1 bp of predicted move
    max_skew:  float = 8.0         # hard cap in either direction (bp)
    # --- optional non-linear damping (helps with outliers) -------------------
    non_linear: bool   = True
    nn_width:   float  = 6.0       # inflection width for tanh when non_linear
    # --- side-and-direction asymmetry tables --------------------------------
    # tightening (<0 prediction)  widening (>0 prediction)
    asym: dict = field(default_factory=lambda: {          # factor applied
        "tightening": {"buy": 1.30, "sell": 0.70},
        "widening"  : {"buy": 0.70, "sell": 1.30}
    })

def _raw_scale(pred: float, cfg: SkewConfig) -> float:
    """
    Turns a prediction magnitude into a *signed* base skew (before asym).
    Uses tanh envelope if cfg.non_linear else linear.
    """
    if cfg.non_linear:
        # tanh gives linear-ish response near 0 and flattens at extremes
        return cfg.base_slope * cfg.max_skew * tanh(pred / cfg.nn_width)
    return cfg.base_slope * pred

def calc_skew(idio_pred_bp: float, side: str, cfg: SkewConfig) -> float:
    direction = "tightening" if idio_pred_bp < 0 else "widening"
    asym_factor = cfg.asym[direction][side.lower()]

    base_skew = _raw_scale(idio_pred_bp, cfg)
    skew = base_skew * asym_factor

    # enforce hard caps
    return max(-cfg.max_skew, min(cfg.max_skew, skew))

def quote_spread(mid_spread: float,
                 idio_pred_bp: float,
                 side: str,
                 cfg: SkewConfig = SkewConfig()):
    """
    Returns the quoted spread and the skew used.
    """
    skew = calc_skew(idio_pred_bp, side, cfg)
    return mid_spread + skew, skew


# ---------------------------------------------------------------------------
# EXAMPLE USAGE
# ---------------------------------------------------------------------------
if __name__ == "__main__":
    mid   = 150.0                 # bp
    pred  = -4.5                  # expect 4.5 bp tightening
    side  = "buy"                 # client wants to buy

    quoted, skew = quote_spread(mid, pred, side)
    print(f"Quoted spread: {quoted:5.2f} bp  (skew {skew:+.2f} bp)")

# RV 
import pandas as pd
import numpy as np

def _one_hot(df: pd.DataFrame, cols: list[str]) -> tuple[np.ndarray, list[str]]:
    """
    Memory‑efficient one‑hot for small (< ~few hundred) cross‑sectional universes.
    Returns design matrix X (n×k) without the intercept (added later).
    """
    mats, names = [], []
    for c in cols:
        codes, uniques = pd.factorize(df[c], sort=True)
        k = uniques.size
        # sparse one‑hot via compare‑broadcast (faster than get_dummies for small n)
        mats.append((codes[:, None] == np.arange(k)).astype(float))
        names.extend([f'{c}={u}' for u in uniques])
    return np.hstack(mats), names

def _cross_sectional_fit(y: np.ndarray, X: np.ndarray) -> np.ndarray:
    """OLS β = (X'X)⁻¹ X'y solved via least‑squares; returns fitted values."""
    # prepend intercept
    Xint = np.hstack((np.ones((X.shape[0], 1)), X))
    coeff, *_ = np.linalg.lstsq(Xint, y, rcond=None)
    return Xint @ coeff  # ŷ

def compute_relative_value_lr(
    df: pd.DataFrame,
    bias_lookback: int = 720,
    shock_lookback: int = 24,
    shock_vol_lookback: int = 168,
    shock_thresh: float = 4.0,
) -> pd.Series:
    """
    Adds a fast cross‑sectional linear regression each hour to isolate 'fair' spread.
    """
    # ----------  housekeeping  ----------
    keys = df.index.str.rsplit('|', n=1, expand=True)
    df = df.assign(isin=keys[0].values,
                   dt=pd.to_datetime(keys[1].values))
    df = df.set_index(['dt', 'isin']).sort_index()

    # pre‑compute numeric liquidity feature
    df['log_amt'] = np.log1p(df['curamount'])

    # ----------  1) Cross‑sectional regression each hour  ----------
    # groupby‑apply keeps the loop in C; each sub‑df is tiny (cross‑section of 100‑400 bonds)
    def _fit_hour(hour_df: pd.DataFrame) -> pd.Series:
        Xcat, _ = _one_hot(hour_df, ['ticker', 'sector', 'tenor', 'seniority'])
        X = np.hstack((Xcat, hour_df[['log_amt']].values))
        y_hat = _cross_sectional_fit(hour_df['mid'].values, X)
        return pd.Series(y_hat, index=hour_df.index.get_level_values('isin'), name='fitted')

    fitted = df.groupby('dt', group_keys=False).apply(_fit_hour)
    df = df.join(fitted)

    resid = df['mid'] - df['fitted']          # cheap/rich vs. linear model

    # ----------  2) De‑bias persistent issuer effects  ----------
    bias = (resid.groupby(level='isin')
                  .rolling(bias_lookback, min_periods=bias_lookback//2)
                  .mean()
                  .droplevel(0))
    adj_resid = resid - bias

    # ----------  3) Shock / credit‑event suppression  ----------
    spread_change = (df['mid'] -
                     df.groupby(level='isin')['mid'].shift(shock_lookback)).abs()
    roll_sigma = (df.groupby(level='isin')['mid']
                    .rolling(shock_vol_lookback, min_periods=shock_vol_lookback//2)
                    .std()
                    .droplevel(0)
                    .replace(0, np.nan))
    shock_flag = (spread_change > shock_thresh * roll_sigma)

    # ----------  4) Final RV score (z‑score of adj_resid) ----------
    cohort_cols = ['sector', 'tenor', 'seniority']
    cohort_std = df.groupby(['dt'] + cohort_cols)['mid'].transform('std').fillna(1.0)
    rv = (adj_resid / cohort_std).where(~shock_flag, 0.0)
    rv.name = 'rv_score'
    return rv

------------------------------------

# RV SPARSE / FAST

import pandas as pd
import numpy as np
from scipy import sparse
from sklearn.feature_extraction import FeatureHasher
from sklearn.linear_model import Ridge

# ----------  helpers  ----------
_hasher = FeatureHasher(n_features=512, input_type='string')  # 512 hashed bins for ticker

def _sparse_one_hot(df: pd.DataFrame, cols: list[str]) -> sparse.csr_matrix:
    """Dense small‑cardinality dummies → sparse CSR."""
    mats = []
    for c in cols:
        codes, _ = pd.factorize(df[c], sort=True)
        n = codes.max() + 1
        row = np.arange(len(df))
        mats.append(sparse.csr_matrix(
            (np.ones_like(codes, dtype=float), (row, codes)),
            shape=(len(df), n)
        ))
    return sparse.hstack(mats, format='csr')

def _fit_hour_fast(hour_df: pd.DataFrame) -> pd.Series:
    """
    Build a sparse design matrix:
        [ one‑hot(sector, tenor, seniority) | hash(ticker) | log(curamount) ]
    and solve ridge → fitted spread.
    """
    # small‑cardinality dummies
    X_base = _sparse_one_hot(hour_df, ['sector', 'tenor', 'seniority'])

    # hashed ticker features (already sparse CSR)
    X_tick = _hasher.transform(hour_df['ticker'])

    # numeric liquidity column
    X_liq  = sparse.csr_matrix(hour_df[['log_amt']].values)

    X = sparse.hstack([X_base, X_tick, X_liq], format='csr')

    y = hour_df['mid'].values
    model = Ridge(alpha=1e‑3, fit_intercept=True, solver='sag', max_iter=100, tol=1e‑3)
    y_hat = model.fit(X, y).predict(X)

    return pd.Series(y_hat, index=hour_df.index.get_level_values('isin'), name='fitted')

# ----------  main pipeline  ----------
def compute_relative_value_large(
    df: pd.DataFrame,
    bias_lookback: int = 720,
    shock_lookback: int = 24,
    shock_vol_lookback: int = 168,
    shock_thresh: float = 4.0,
) -> pd.Series:
    """
    Relative‑value score identical to previous version,
    but cross‑sectional fair value is estimated with a sparse hashed‑feature ridge
    => scales to 10 k+ bonds per hour.
    """
    keys = df.index.str.rsplit('|', n=1, expand=True)
    df = df.assign(isin=keys[0].values,
                   dt=pd.to_datetime(keys[1].values),
                   ticker=keys[0].str[:6])          # e.g. first part of ISIN as ticker
    df = df.set_index(['dt', 'isin']).sort_index()
    df['log_amt'] = np.log1p(df['curamount'])

    # 1) cross‑sectional fit (parallelisable if desired)
    fitted = df.groupby('dt', group_keys=False).apply(_fit_hour_fast)
    df = df.join(fitted)

    resid = df['mid'] - df['fitted']

    # 2) issuer bias removal
    bias = (resid.groupby(level='isin')
                  .rolling(bias_lookback, min_periods=bias_lookback//2)
                  .mean()
                  .droplevel(0))
    adj_resid = resid - bias

    # 3) credit‑event suppression
    spread_change = (df['mid'] -
                     df.groupby(level='isin')['mid'].shift(shock_lookback)).abs()
    roll_sigma = (df.groupby(level='isin')['mid']
                    .rolling(shock_vol_lookback, min_periods=shock_vol_lookback//2)
                    .std()
                    .droplevel(0)
                    .replace(0, np.nan))
    shock_flag = (spread_change > shock_thresh * roll_sigma)

    # 4) z‑score within cohort & output
    cohort_std = df.groupby(['dt', 'sector', 'tenor', 'seniority'])['mid'] \
                   .transform('std').fillna(1.0)

    rv = (adj_resid / cohort_std).where(~shock_flag, 0.0)
    rv.name = 'rv_score'
    return rv


-------------------------------------

import numpy as np

def scaled_sigma(z, p, sigma0, z_ref=3.0, alpha=0.8, beta=0.6):
    """
    z       : ndarray   # vol-adj return, <0 tighten, >0 widen
    p       : ndarray   # prob(tighten)
    sigma0  : ndarray   # baseline σ (bp)
    z_ref   : float     # cap |z| at z_ref for strength calc
    alpha   : float     # boost factor on agreement
    beta    : float     # shrink factor on disagreement
    """
    sign_z   = np.sign(z)
    sign_p   = np.where(p > 0.5, -1.0, 1.0)

    A = 0.5 * (1 + sign_z * sign_p)            # 1 agree, 0 disagree
    C = 2.0 * np.abs(p - 0.5)                  # confidence
    S = np.minimum(1.0, np.abs(z) / z_ref)     # regressor strength

    scale = 1 + alpha * A * C * S - beta * (1 - A) * C
    return sigma0 * scale


def gross_return(z, p, sigma0, **kw):
    sig = scaled_sigma(z, p, sigma0, **kw)
    return np.sign(z) * np.abs(z) * sig

# ================================================================
# 0.  Imports & basic config
# ================================================================
import numpy as np
import pandas as pd
from pathlib import Path
from typing import List, Tuple, Dict, Sequence
import matplotlib.pyplot as plt
import seaborn as sns           # nicer default look for the plots

from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import RidgeCV
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    mean_squared_error,
    mean_absolute_error,
    r2_score,
)
from sklearn.model_selection import TimeSeriesSplit

plt.switch_backend("agg")       # comment-out if running interactively

# ------------------------------------------------
# Directory where model-performance snapshots will be written
METRIC_DIR = Path("model_metrics")
METRIC_DIR.mkdir(exist_ok=True)

# Seed for any stochastic steps (e.g. CV shard order)
RNG = np.random.default_rng(seed=17)


# ================================================================
# 1.  FEATURE-ENGINEERING HELPERS
# ================================================================
def trailingVol(series: pd.Series, windowLen: int = 10) -> pd.Series:
    """
    Rolling (windowLen-day) standard deviation, rescaled to the same horizon.
    Used as a simple backward-looking volatility proxy.
    """
    return (
        series.rolling(windowLen, min_periods=windowLen)
              .std()
              * np.sqrt(windowLen)
    )


def ewmaVol(series: pd.Series, spanDays: int = 20, horizonLen: int = 10) -> pd.Series:
    """
    Exponentially-weighted stdev with half-life ~= spanDays.
    Also rescaled to 'horizonLen' for 1→N-day conversion.
    """
    ewma = series.ewm(span=spanDays, adjust=False).std()
    return ewma * np.sqrt(horizonLen)


def relBidOfferWidth(bid: pd.Series, offer: pd.Series) -> pd.Series:
    """Bid-offer width in bp (offer – bid)."""
    return offer - bid


def pctChange(series: pd.Series, periods: int = 1) -> pd.Series:
    """Convenience wrapper for pandas pct_change that keeps camelCase naming."""
    return series.pct_change(periods)


def buildFeatures(df: pd.DataFrame) -> pd.DataFrame:
    """
    Main feature factory.
    Adds volatility, momentum, liquidity, macro, and sector dummies
    that the forward-vol model will consume.
    """
    out = df.copy()

    # --- 1-day idiosyncratic spread change ---------------------
    out["d_idio_spread"] = out.groupby("cusip")["idio_spread_bp"].diff()

    # --- Volatility signatures --------------------------------
    out["trail_vol_10d"] = (
        out.groupby("cusip")["d_idio_spread"].apply(trailingVol, windowLen=10)
    )
    out["trail_vol_60d"] = (
        out.groupby("cusip")["d_idio_spread"].apply(trailingVol, windowLen=60)
    )
    out["ewma_vol_20d"] = (
        out.groupby("cusip")["d_idio_spread"].apply(ewmaVol,
                                                    spanDays=20,
                                                    horizonLen=10)
    )

    # --- Short-term momentum (captures mean-reversion vs trend)
    out["mom_5d"] = out.groupby("cusip")["idio_spread_bp"].apply(lambda s: s.diff(5))

    # --- Micro-liquidity proxies -------------------------------
    out["bo_width_bp"] = relBidOfferWidth(out["bid_bp"], out["offer_bp"])
    out["rfq_cnt_5d"] = (
        out.groupby("cusip")["rfq_flag"]
           .rolling(5).sum().reset_index(level=0, drop=True)
    )

    # --- Systemic credit / rates vol proxies ------------------
    out["cdx_ig_impvol"] = out["cdx_ig_price"].rolling(5).std()
    out["move_idx"] = out["ust_yield_2y"].rolling(5).std()

    # --- ETF liquidity stress (discount = ETF < iNAV) ----------
    out["etf_disc_5d"] = out["etf_discount_bp"].rolling(5).mean()

    # --- Sparse sector one-hot encoding ------------------------
    sectorDummies = pd.get_dummies(out["issuer_sector"],
                                   prefix="sec",
                                   dtype=np.int8)
    out = pd.concat([out, sectorDummies], axis=1)

    # --- Rating-watch events ----------------------------------
    out["on_watch"] = out["rating_watch_flag"].fillna(0)

    return out


# ================================================================
# 2.  DATE-BLOCK GENERATOR
# ================================================================
DateBlock = Tuple[str, str, str, str]  # (trainStart, trainEnd, testStart, testEnd)

def autoGenerateDateBlocks(
    df: pd.DataFrame,
    nModels: int,
    trainDays: int,
    testDays: int,
    dateCol: str = "date",
) -> List[DateBlock]:
    """
    Chop the panel into sequential, *non-overlapping* train/test chunks.

    Exits early if it runs out of data.
    """
    dfDates = pd.to_datetime(df[dateCol])
    start = dfDates.min().normalize()

    blocks: List[DateBlock] = []
    for _ in range(nModels):
        trainStart = start
        trainEnd   = start + pd.Timedelta(days=trainDays - 1)
        testStart  = trainEnd + pd.Timedelta(days=1)
        testEnd    = testStart + pd.Timedelta(days=testDays - 1)

        if testEnd > dfDates.max():
            break   # not enough data left for another full block

        blocks.append((
            trainStart.strftime("%Y-%m-%d"),
            trainEnd.strftime("%Y-%m-%d"),
            testStart.strftime("%Y-%m-%d"),
            testEnd.strftime("%Y-%m-%d"),
        ))

        # move pointer to the day after this test block
        start = testEnd + pd.Timedelta(days=1)

    if not blocks:
        raise ValueError("No non-overlapping blocks could be produced.")

    return blocks


# ================================================================
# 3.  MODEL-TRAINING UTILITIES
# ================================================================
def makeDateMasks(
    df: pd.DataFrame,
    split: DateBlock,
    dateCol: str = "date",
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Produce boolean indexers for the train/test periods of a single block.
    """
    trS, trE, teS, teE = [pd.to_datetime(d) for d in split]
    maskTrain = (df[dateCol] >= trS) & (df[dateCol] <= trE)
    maskTest  = (df[dateCol] >= teS) & (df[dateCol] <= teE)

    # Safety check – should *never* overlap for one model
    if (maskTrain & maskTest).any():
        raise ValueError(f"Train/Test overlap in split {split}")

    return maskTrain.values, maskTest.values


def trainRidgeGeneric(
    dfFeat: pd.DataFrame,
    featureCols: Sequence[str],
    targetCol: str,
    dateSplits: List[DateBlock],
    alphas: np.ndarray = np.logspace(-3, 2, 20),
    modelName: str = "ridgeGeneric",
    storeMetrics: bool = True,
) -> Dict[str, Dict]:
    """
    Loop over dateSplits, fit a RidgeCV model in each,
    evaluate OOS performance, and (optionally) persist metrics.
    """
    metrics: Dict[str, Dict] = {}
    XAll = dfFeat[featureCols].values
    yAll = np.log(dfFeat[targetCol].clip(lower=1e-5))  # log stabilises variance

    for i, split in enumerate(dateSplits, 1):
        maskTrain, maskTest = makeDateMasks(dfFeat, split)

        # sklearn Pipeline: standardise → ridge with CV over 'alphas'
        # Create a time-aware CV splitter. Ensure data is sorted by time
        tscv = TimeSeriesSplit(n_splits=3)
        model = Pipeline([
            ("scaler", StandardScaler()),
            ("ridge", RidgeCV(alphas=alphas,
                              scoring="neg_mean_squared_error",
                              cv=tscv)),
        ]).fit(XAll[maskTrain], yAll[maskTrain])

        # Predictions
        yhatTrain = model.predict(XAll[maskTrain])
        yhatTest  = model.predict(XAll[maskTest])

        # Common regression diagnostics
        rmseTr = mean_squared_error(yAll[maskTrain], yhatTrain, squared=False)
        rmseTe = mean_squared_error(yAll[maskTest],  yhatTest,  squared=False)
        maeTe  = mean_absolute_error(yAll[maskTest], yhatTest)
        r2Te   = r2_score(yAll[maskTest], yhatTest)

        key = f"{modelName}_split{i}"
        metrics[key] = {
            "alpha":       model.named_steps["ridge"].alpha_,
            "rmse_train":  rmseTr,
            "rmse_test":   rmseTe,
            "mae_test":    maeTe,
            "r2_test":     r2Te,
            "n_train":     int(maskTrain.sum()),
            "n_test":      int(maskTest.sum()),
            "train_start": split[0],
            "train_end":   split[1],
            "test_start":  split[2],
            "test_end":    split[3],
            "features":    list(featureCols),
        }

    # Persist as Parquet for later dashboards
    if storeMetrics:
        pd.DataFrame.from_dict(metrics, orient="index").to_parquet(
            METRIC_DIR / f"{modelName}_metrics.parquet"
        )

    return metrics


# ================================================================
# 4.  VISUALISATION HELPERS
# ================================================================
def plotMetricOverTime(
    metrics: Dict[str, Dict],
    metricKey: str = "r2_test",
    title: str | None = None,
):
    """
    Line plot: metricKey vs. train_end date across all splits.
    Keeps formatting generic so any scalar metric can be plotted.
    """
    dfM = pd.DataFrame.from_dict(metrics, orient="index")
    dfM["train_end"] = pd.to_datetime(dfM["train_end"])
    dfM = dfM.sort_values("train_end")

    plt.figure(figsize=(6, 4))
    sns.lineplot(data=dfM, x="train_end", y=metricKey, marker="o")
    sns.despine()
    plt.xlabel("Train period end")
    plt.ylabel(metricKey.replace("_", " ").title())
    plt.title(title or f"{metricKey} over successive date blocks")
    plt.tight_layout()
    plt.show()


def plotMultipleMetrics(
    metrics: Dict[str, Dict],
    metricKeys: Sequence[str] = ("rmse_test", "mae_test", "r2_test"),
):
    """
    Convenience wrapper: mini-dashboard of several metrics stacked vertically.
    """
    dfM = pd.DataFrame.from_dict(metrics, orient="index")
    dfM["train_end"] = pd.to_datetime(dfM["train_end"])
    dfM = dfM.sort_values("train_end")

    n = len(metricKeys)
    fig, axes = plt.subplots(nrows=n, figsize=(6, 3 * n), sharex=True)

    for ax, k in zip(axes, metricKeys):
        sns.lineplot(data=dfM, x="train_end", y=k, marker="o", ax=ax)
        ax.set_ylabel(k.replace("_", " ").title())
        ax.set_xlabel("")
        sns.despine(ax=ax)

    axes[-1].set_xlabel("Train period end")
    plt.tight_layout()
    plt.show()


# ================================================================
# 5.  END-TO-END USAGE EXAMPLE
# ================================================================
if __name__ == "__main__":

    # ------------------------------------------------------------
    # 5.1  Load raw daily CUSIP-level panel
    raw = pd.read_parquet("bond_daily_raw.parquet")
    raw["date"] = pd.to_datetime(raw["date"])

    # ------------------------------------------------------------
    # 5.2  Create engineered features
    feat = buildFeatures(raw)

    # Core feature list + automatically captured sector dummies
    FEATURES = [
        # volatility fingerprints
        "trail_vol_10d", "trail_vol_60d", "ewma_vol_20d",
        # momentum
        "mom_5d",
        # systemic vols
        "cdx_ig_impvol", "move_idx",
        # liquidity & stress
        "bo_width_bp", "rfq_cnt_5d", "etf_disc_5d",
        # event indicator
        "on_watch",
    ] + [c for c in feat.columns if c.startswith("sec_")]

    TARGET = "real_10d_vol"   # realised forward ten-day volatility in bp

    # Keep rows with complete data only
    feat = feat.dropna(subset=FEATURES + [TARGET]).reset_index(drop=True)

    # ------------------------------------------------------------
    # 5.3  Set up sequential (non-overlapping) train/test slices
    dateBlocks = autoGenerateDateBlocks(
        df=feat,
        nModels=4,       # train four independent models
        trainDays=365,   # each sees one year of history
        testDays=180,    # and is evaluated on the next ~6 months
        dateCol="date",
    )
    print("Date blocks:", dateBlocks)

    # ------------------------------------------------------------
    # 5.4  Train ridge on each block & collect diagnostics
    results = trainRidgeGeneric(
        dfFeat=feat,
        featureCols=FEATURES,
        targetCol=TARGET,
        dateSplits=dateBlocks,
        modelName="fwdVolRidge",
    )

    # ------------------------------------------------------------
    # 5.5  Quick visual health-check
    plotMetricOverTime(results, metricKey="r2_test",
                       title="Out-of-sample R² of Forward-Vol Ridge")
    plotMultipleMetrics(results)

--------------------------------
--------------------------------

• ε̂ vs gross ........ 0.08–0.12
• ε̂ + βF̂  .......... 0.12–0.18
• γ-scaled ........... 0.14–0.20

Pearson IC  ≈  Spearman IC  ± 0.01

from scipy.stats import spearmanr, pearsonr

rank_ic     = spearmanr(forecast_vec, realised_vec).correlation  # Spearman
linear_ic   = pearsonr (forecast_vec, realised_vec)[0]           # Pearson

import pandas as pd
import numpy as np

# --- assume you already have these three daily Series -------------
# idx = DatetimeIndex of trade dates
# each Series is length T (= # days) with NaN where data not available
eps_hat    = ...   # idiosyncratic % forecast, index = dates
sys_hat    = ...   # systematic  % forecast, index = dates
gross_real = ...   # realised     % move,   index = dates

# --- put them in one DataFrame for convenience -------------------
df = pd.DataFrame({
        'y':   gross_real,
        'x1':  eps_hat,
        'x2':  sys_hat
     }).dropna()

window = 504                       # ~ 2 years of trading days
coeffs = pd.DataFrame(index=df.index, columns=['gamma1','gamma2'])

# --- rolling OLS ---------------------------------------------------
for end in range(window, len(df)):
    slice_ = df.iloc[end-window:end]
    X = slice_[['x1','x2']].values
    y = slice_['y'].values
    # add no intercept → y = gamma1*x1 + gamma2*x2
    gamma, *_ = np.linalg.lstsq(X, y, rcond=None)
    coeffs.iloc[end] = gamma      # store (gamma1, gamma2)

# forward-fill the earlier rows so we have coeffs for every date
coeffs = coeffs.ffill()

# --- apply calibrated forecast ------------------------------------
df['y_cal'] = coeffs['gamma1']*df['x1'] + coeffs['gamma2']*df['x2']

# --- sanity check: rolling rank IC --------------------------------
ic_raw = df['y'].rolling(126).corr(df['x1']+df['x2'])     # ~6-mo window
ic_cal = df['y'].rolling(126).corr(df['y_cal'])
print('Average IC raw :', ic_raw.mean())
print('Average IC cal :', ic_cal.mean())


import pandas as pd
from scipy.stats import spearmanr
import numpy as np

# idio_pred and idio_real are DataFrames: index = dates, columns = bonds
def daily_rank_ic(idio_pred, idio_real):
    out = []
    for date in idio_pred.index:
        f = idio_pred.loc[date].values      # forecasts
        r = idio_real.loc[date].values      # realised idio returns
        mask = ~np.isnan(f) & ~np.isnan(r)
        if mask.sum() > 3:                  # need at least 4 names
            ic = spearmanr(f[mask], r[mask]).correlation
            out.append(ic)
        else:
            out.append(np.nan)
    return pd.Series(out, index=idio_pred.index)

rank_ic = daily_rank_ic(idio_pred, idio_real)
print("Average daily Rank IC:", rank_ic.mean())


# eps_hat  = N×T matrix of idio forecasts
# gross_hat= eps_hat + beta @ F_hat    (systematic added)
# realised_epsilon = realised_idio_ret
# realised_gross   = realised_raw_ret_pct

from scipy.stats import spearmanr

def rank_ic(forecast, realised):
    ic = [spearmanr(forecast[:,t], realised[:,t]).correlation
          for t in range(forecast.shape[1])]
    return np.nanmean(ic)

ic_eps_vs_eps   = rank_ic(eps_hat, realised_epsilon)
ic_eps_vs_gross = rank_ic(eps_hat, realised_gross)
ic_full_vs_gross= rank_ic(gross_hat, realised_gross)

print(ic_eps_vs_eps, ic_eps_vs_gross, ic_full_vs_gross)


def cross_sectional_ridge(eps_hat: np.ndarray,
                          beta:    np.ndarray,
                          lam:     float | None = None,
                          return_gross: bool = True):
    """
    Cross-sectional ridge solve:
        argmin_F  || eps_hat + beta @ F ||^2 + lam * ||F||^2

    Parameters
    ----------
    eps_hat : (N,)  idiosyncratic %-return predictions
    beta    : (N, L) factor-loading matrix
    lam     : ridge penalty; default = 1e-4 * N   (good first guess)
    return_gross : also return eps_hat + beta @ F_hat if True

    Returns
    -------
    F_hat : (L,)  inferred factor % returns
    gross_pred  (optional) : (N,) gross % forecast per bond
    """
    eps_hat = np.asarray(eps_hat, dtype=float)
    beta    = np.asarray(beta,    dtype=float)
    N, L    = beta.shape

    # sensible default for lambda
    if lam is None:
        lam = 1e-4 * N

    A = beta.T @ beta + lam * np.eye(L)       # (L,L)
    b = -beta.T @ eps_hat                     # (L,)

    F_hat = np.linalg.solve(A, b)             # (L,)

    if return_gross:
        gross_pred = eps_hat + beta @ F_hat   # (N,)
        return F_hat, gross_pred
    return F_hat

# --- inputs -----------------------------------------------------------
eps_hat = np.array([0.0040, 0.0035, 0.0001, -0.0002])   # % changes

beta = np.array([[1, 1, 0],     # bond A
                 [1, 1, 0],     # bond B
                 [1, 0, 1],     # bond C
                 [1, 0, 1]])    # bond D

lam = 0.004          # λ = 0.001 * N  (N = 4)

# --- solve ------------------------------------------------------------
F_hat, gross = cross_sectional_ridge(eps_hat, beta, lam)

print("Factor returns (10-day %):")
print(f"  Market   : {F_hat[0]: .6%}")
print(f"  Energy   : {F_hat[1]: .6%}")
print(f"  Utilities: {F_hat[2]: .6%}")

print("\nBond-level gross % forecast:")
for i, g in enumerate(gross, 1):
    print(f"  Bond {i}: {g: .6%}")


import pandas as pd
import numpy as np

def calc_distribution_averages(df1, df2, n_buckets=10):
    """
    Calculate average values of df2 across the distribution of df1,
    using fully vectorized operations for maximum performance.
    
    Parameters:
    df1, df2: pandas DataFrames with same shape
    n_buckets: number of buckets to create
    
    Returns:
    DataFrame with buckets as index and average values as columns
    """
    # Calculate quantile boundaries across all values
    all_values = df1.values.ravel()
    boundaries = np.percentile(all_values, 
                             np.linspace(0, 100, n_buckets + 1),
                             method='linear')
    
    # Vectorized bucket assignment for entire DataFrame
    bucket_indices = np.digitize(df1.values, boundaries[1:-1])
    
    # Create a DataFrame of bucket assignments
    buckets_df = pd.DataFrame(bucket_indices, 
                             index=df1.index,
                             columns=df1.columns)
    
    # Stack both DataFrames to long format for vectorized groupby
    stacked_buckets = buckets_df.stack()
    stacked_values = df2.stack()
    
    # Perform groupby operation once for all columns
    result = (pd.DataFrame({'bucket': stacked_buckets, 'value': stacked_values})
             .groupby(['bucket'])['value']
             .mean()
             .reset_index())
    
    # Reshape back to original format
    result = (result.pivot(columns='bucket', values='value')
             .rename(columns={i: f'Q{i+1}' for i in range(n_buckets)})
             .T)
    
    return result

import pandas as pd
import numpy as np

def process_group(df_group):
    """
    Process a single ISIN group (already sorted by time) and simulate the sequential
    impact of candidate trades. Returns extra columns:
      - risk_reducing: bool indicating if that trade would have reduced risk
      - candidate_effect: using the 'size' column (which equals posPost - posPre)
      - simulated_start_before: simulation state (starting position) before the trade
      - simulated_final_after: simulation state (final position) after the trade if applied
      - abs_improvement: improvement in |position| from applying this trade
    """
    # Sort by time.
    df_group = df_group.sort_values('time').copy()
    n = len(df_group)
    
    # Extract arrays for speed.
    times = df_group['time'].values
    posPre_arr = df_group['posPre'].values
    posPost_arr = df_group['posPost'].values
    doneStatus_arr = df_group['status'].values
    size_arr = df_group['size'].values  # Using the provided size column directly

    # Use size as the candidate effect.
    cand_effect = size_arr.copy()

    # Allocate output arrays.
    risk_reducing = np.zeros(n, dtype=bool)
    simulated_start_before = np.empty(n, dtype=posPre_arr.dtype)
    simulated_final_after = np.empty(n, dtype=posPre_arr.dtype)
    abs_improvement = np.zeros(n, dtype=posPre_arr.dtype)
    
    # Determine the baseline simulation state.
    # Look for the last DONE trade.
    final_trade_index = None
    for i in range(n):
        if doneStatus_arr[i] == 'DONE':
            final_trade_index = i  # will update to the last DONE trade index

    if final_trade_index is not None:
        # For a DONE trade, use its posPre as the baseline start,
        # and use its posPost directly as the baseline final.
        fixed_effect = posPost_arr[final_trade_index] - posPre_arr[final_trade_index]
        baseline_start = posPre_arr[final_trade_index]
        baseline_final = posPost_arr[final_trade_index]
    else:
        fixed_effect = 0.0
        baseline_start = posPre_arr[0]
        baseline_final = baseline_start

    # Initialize simulation state.
    current_start = baseline_start
    current_final = baseline_final

    # Process each row sequentially.
    for i in range(n):
        simulated_start_before[i] = current_start
        # In the DONE-case, only simulate for trades before the final DONE trade.
        if (final_trade_index is not None) and (i >= final_trade_index):
            # For trades at or after the final DONE trade, we do not simulate.
            simulated_final_after[i] = current_final
            abs_improvement[i] = 0.0
            risk_reducing[i] = False
        else:
            new_start = current_start + cand_effect[i]
            # In the DONE-case, the final position is updated by adding the fixed effect.
            # In the no-DONE-case, fixed_effect is 0.
            new_final = new_start + fixed_effect
            improvement = abs(current_final) - abs(new_final)
            if improvement > 0:
                risk_reducing[i] = True
                # Update the simulation state.
                current_start = new_start
                current_final = new_final
                abs_improvement[i] = improvement
            else:
                risk_reducing[i] = False
                abs_improvement[i] = 0.0
            simulated_final_after[i] = current_final

    # Attach the computed arrays as new columns.
    df_group['risk_reducing'] = risk_reducing
    df_group['candidate_effect'] = cand_effect
    df_group['simulated_start_before'] = simulated_start_before
    df_group['simulated_final_after'] = simulated_final_after
    df_group['abs_improvement'] = abs_improvement

    return df_group

def add_risk_reducing_info(df):
    """
    Process the entire dataframe by ISIN and return the dataframe with additional
    columns that indicate for each row if that trade would have been risk reducing and
    how that conclusion was reached.
    """
    # Process by ISIN groups.
    df_with_info = df.groupby('isin', group_keys=False).apply(process_group)
    return df_with_info

import numpy as np

