• ε̂ vs gross ........ 0.08–0.12
• ε̂ + βF̂  .......... 0.12–0.18
• γ-scaled ........... 0.14–0.20

Pearson IC  ≈  Spearman IC  ± 0.01

from scipy.stats import spearmanr, pearsonr

rank_ic     = spearmanr(forecast_vec, realised_vec).correlation  # Spearman
linear_ic   = pearsonr (forecast_vec, realised_vec)[0]           # Pearson

import pandas as pd
import numpy as np

# --- assume you already have these three daily Series -------------
# idx = DatetimeIndex of trade dates
# each Series is length T (= # days) with NaN where data not available
eps_hat    = ...   # idiosyncratic % forecast, index = dates
sys_hat    = ...   # systematic  % forecast, index = dates
gross_real = ...   # realised     % move,   index = dates

# --- put them in one DataFrame for convenience -------------------
df = pd.DataFrame({
        'y':   gross_real,
        'x1':  eps_hat,
        'x2':  sys_hat
     }).dropna()

window = 504                       # ~ 2 years of trading days
coeffs = pd.DataFrame(index=df.index, columns=['gamma1','gamma2'])

# --- rolling OLS ---------------------------------------------------
for end in range(window, len(df)):
    slice_ = df.iloc[end-window:end]
    X = slice_[['x1','x2']].values
    y = slice_['y'].values
    # add no intercept → y = gamma1*x1 + gamma2*x2
    gamma, *_ = np.linalg.lstsq(X, y, rcond=None)
    coeffs.iloc[end] = gamma      # store (gamma1, gamma2)

# forward-fill the earlier rows so we have coeffs for every date
coeffs = coeffs.ffill()

# --- apply calibrated forecast ------------------------------------
df['y_cal'] = coeffs['gamma1']*df['x1'] + coeffs['gamma2']*df['x2']

# --- sanity check: rolling rank IC --------------------------------
ic_raw = df['y'].rolling(126).corr(df['x1']+df['x2'])     # ~6-mo window
ic_cal = df['y'].rolling(126).corr(df['y_cal'])
print('Average IC raw :', ic_raw.mean())
print('Average IC cal :', ic_cal.mean())


import pandas as pd
from scipy.stats import spearmanr
import numpy as np

# idio_pred and idio_real are DataFrames: index = dates, columns = bonds
def daily_rank_ic(idio_pred, idio_real):
    out = []
    for date in idio_pred.index:
        f = idio_pred.loc[date].values      # forecasts
        r = idio_real.loc[date].values      # realised idio returns
        mask = ~np.isnan(f) & ~np.isnan(r)
        if mask.sum() > 3:                  # need at least 4 names
            ic = spearmanr(f[mask], r[mask]).correlation
            out.append(ic)
        else:
            out.append(np.nan)
    return pd.Series(out, index=idio_pred.index)

rank_ic = daily_rank_ic(idio_pred, idio_real)
print("Average daily Rank IC:", rank_ic.mean())


# eps_hat  = N×T matrix of idio forecasts
# gross_hat= eps_hat + beta @ F_hat    (systematic added)
# realised_epsilon = realised_idio_ret
# realised_gross   = realised_raw_ret_pct

from scipy.stats import spearmanr

def rank_ic(forecast, realised):
    ic = [spearmanr(forecast[:,t], realised[:,t]).correlation
          for t in range(forecast.shape[1])]
    return np.nanmean(ic)

ic_eps_vs_eps   = rank_ic(eps_hat, realised_epsilon)
ic_eps_vs_gross = rank_ic(eps_hat, realised_gross)
ic_full_vs_gross= rank_ic(gross_hat, realised_gross)

print(ic_eps_vs_eps, ic_eps_vs_gross, ic_full_vs_gross)


def cross_sectional_ridge(eps_hat: np.ndarray,
                          beta:    np.ndarray,
                          lam:     float | None = None,
                          return_gross: bool = True):
    """
    Cross-sectional ridge solve:
        argmin_F  || eps_hat + beta @ F ||^2 + lam * ||F||^2

    Parameters
    ----------
    eps_hat : (N,)  idiosyncratic %-return predictions
    beta    : (N, L) factor-loading matrix
    lam     : ridge penalty; default = 1e-4 * N   (good first guess)
    return_gross : also return eps_hat + beta @ F_hat if True

    Returns
    -------
    F_hat : (L,)  inferred factor % returns
    gross_pred  (optional) : (N,) gross % forecast per bond
    """
    eps_hat = np.asarray(eps_hat, dtype=float)
    beta    = np.asarray(beta,    dtype=float)
    N, L    = beta.shape

    # sensible default for lambda
    if lam is None:
        lam = 1e-4 * N

    A = beta.T @ beta + lam * np.eye(L)       # (L,L)
    b = -beta.T @ eps_hat                     # (L,)

    F_hat = np.linalg.solve(A, b)             # (L,)

    if return_gross:
        gross_pred = eps_hat + beta @ F_hat   # (N,)
        return F_hat, gross_pred
    return F_hat

# --- inputs -----------------------------------------------------------
eps_hat = np.array([0.0040, 0.0035, 0.0001, -0.0002])   # % changes

beta = np.array([[1, 1, 0],     # bond A
                 [1, 1, 0],     # bond B
                 [1, 0, 1],     # bond C
                 [1, 0, 1]])    # bond D

lam = 0.004          # λ = 0.001 * N  (N = 4)

# --- solve ------------------------------------------------------------
F_hat, gross = cross_sectional_ridge(eps_hat, beta, lam)

print("Factor returns (10-day %):")
print(f"  Market   : {F_hat[0]: .6%}")
print(f"  Energy   : {F_hat[1]: .6%}")
print(f"  Utilities: {F_hat[2]: .6%}")

print("\nBond-level gross % forecast:")
for i, g in enumerate(gross, 1):
    print(f"  Bond {i}: {g: .6%}")


import pandas as pd
import numpy as np

def calc_distribution_averages(df1, df2, n_buckets=10):
    """
    Calculate average values of df2 across the distribution of df1,
    using fully vectorized operations for maximum performance.
    
    Parameters:
    df1, df2: pandas DataFrames with same shape
    n_buckets: number of buckets to create
    
    Returns:
    DataFrame with buckets as index and average values as columns
    """
    # Calculate quantile boundaries across all values
    all_values = df1.values.ravel()
    boundaries = np.percentile(all_values, 
                             np.linspace(0, 100, n_buckets + 1),
                             method='linear')
    
    # Vectorized bucket assignment for entire DataFrame
    bucket_indices = np.digitize(df1.values, boundaries[1:-1])
    
    # Create a DataFrame of bucket assignments
    buckets_df = pd.DataFrame(bucket_indices, 
                             index=df1.index,
                             columns=df1.columns)
    
    # Stack both DataFrames to long format for vectorized groupby
    stacked_buckets = buckets_df.stack()
    stacked_values = df2.stack()
    
    # Perform groupby operation once for all columns
    result = (pd.DataFrame({'bucket': stacked_buckets, 'value': stacked_values})
             .groupby(['bucket'])['value']
             .mean()
             .reset_index())
    
    # Reshape back to original format
    result = (result.pivot(columns='bucket', values='value')
             .rename(columns={i: f'Q{i+1}' for i in range(n_buckets)})
             .T)
    
    return result

import pandas as pd
import numpy as np

def process_group(df_group):
    """
    Process a single ISIN group (already sorted by time) and simulate the sequential
    impact of candidate trades. Returns extra columns:
      - risk_reducing: bool indicating if that trade would have reduced risk
      - candidate_effect: using the 'size' column (which equals posPost - posPre)
      - simulated_start_before: simulation state (starting position) before the trade
      - simulated_final_after: simulation state (final position) after the trade if applied
      - abs_improvement: improvement in |position| from applying this trade
    """
    # Sort by time.
    df_group = df_group.sort_values('time').copy()
    n = len(df_group)
    
    # Extract arrays for speed.
    times = df_group['time'].values
    posPre_arr = df_group['posPre'].values
    posPost_arr = df_group['posPost'].values
    doneStatus_arr = df_group['status'].values
    size_arr = df_group['size'].values  # Using the provided size column directly

    # Use size as the candidate effect.
    cand_effect = size_arr.copy()

    # Allocate output arrays.
    risk_reducing = np.zeros(n, dtype=bool)
    simulated_start_before = np.empty(n, dtype=posPre_arr.dtype)
    simulated_final_after = np.empty(n, dtype=posPre_arr.dtype)
    abs_improvement = np.zeros(n, dtype=posPre_arr.dtype)
    
    # Determine the baseline simulation state.
    # Look for the last DONE trade.
    final_trade_index = None
    for i in range(n):
        if doneStatus_arr[i] == 'DONE':
            final_trade_index = i  # will update to the last DONE trade index

    if final_trade_index is not None:
        # For a DONE trade, use its posPre as the baseline start,
        # and use its posPost directly as the baseline final.
        fixed_effect = posPost_arr[final_trade_index] - posPre_arr[final_trade_index]
        baseline_start = posPre_arr[final_trade_index]
        baseline_final = posPost_arr[final_trade_index]
    else:
        fixed_effect = 0.0
        baseline_start = posPre_arr[0]
        baseline_final = baseline_start

    # Initialize simulation state.
    current_start = baseline_start
    current_final = baseline_final

    # Process each row sequentially.
    for i in range(n):
        simulated_start_before[i] = current_start
        # In the DONE-case, only simulate for trades before the final DONE trade.
        if (final_trade_index is not None) and (i >= final_trade_index):
            # For trades at or after the final DONE trade, we do not simulate.
            simulated_final_after[i] = current_final
            abs_improvement[i] = 0.0
            risk_reducing[i] = False
        else:
            new_start = current_start + cand_effect[i]
            # In the DONE-case, the final position is updated by adding the fixed effect.
            # In the no-DONE-case, fixed_effect is 0.
            new_final = new_start + fixed_effect
            improvement = abs(current_final) - abs(new_final)
            if improvement > 0:
                risk_reducing[i] = True
                # Update the simulation state.
                current_start = new_start
                current_final = new_final
                abs_improvement[i] = improvement
            else:
                risk_reducing[i] = False
                abs_improvement[i] = 0.0
            simulated_final_after[i] = current_final

    # Attach the computed arrays as new columns.
    df_group['risk_reducing'] = risk_reducing
    df_group['candidate_effect'] = cand_effect
    df_group['simulated_start_before'] = simulated_start_before
    df_group['simulated_final_after'] = simulated_final_after
    df_group['abs_improvement'] = abs_improvement

    return df_group

def add_risk_reducing_info(df):
    """
    Process the entire dataframe by ISIN and return the dataframe with additional
    columns that indicate for each row if that trade would have been risk reducing and
    how that conclusion was reached.
    """
    # Process by ISIN groups.
    df_with_info = df.groupby('isin', group_keys=False).apply(process_group)
    return df_with_info

import numpy as np

