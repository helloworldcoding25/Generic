print("hello world")

import pandas as pd
import dask.dataframe as dd
import vaex
import numpy as np
import pyarrow as pa
import modin.pandas as mpd
from pathlib import Path

def optimize_pandas_df(df):
    """
    Optimize pandas DataFrame memory usage by downcasting numeric columns
    and categorizing string columns with low cardinality.
    """
    for col in df.columns:
        # Downcast numeric columns
        if pd.api.types.is_numeric_dtype(df[col]):
            if pd.api.types.is_integer_dtype(df[col]):
                df[col] = pd.to_numeric(df[col], downcast='integer')
            else:
                df[col] = pd.to_numeric(df[col], downcast='float')
        # Convert string columns with low cardinality to categorical
        elif pd.api.types.is_string_dtype(df[col]):
            num_unique = df[col].nunique()
            if num_unique / len(df) < 0.1:  # If less than 10% unique values
                df[col] = df[col].astype('category')
    return df

def process_large_data_pandas(df1_path, df2_path, filter_conditions):
    """
    Process large DataFrames using chunked reading with pandas
    """
    chunk_size = 1_000_000
    reader1 = pd.read_csv(df1_path, chunksize=chunk_size)
    reader2 = pd.read_csv(df2_path, chunksize=chunk_size)
    
    # Process chunks and concatenate results
    results = []
    for chunk1, chunk2 in zip(reader1, reader2):
        # Optimize memory usage
        chunk1 = optimize_pandas_df(chunk1)
        chunk2 = optimize_pandas_df(chunk2)
        
        # Merge chunks
        merged = pd.merge(chunk1, chunk2, on='key_column')
        
        # Apply filters
        for condition in filter_conditions:
            merged = merged.query(condition)
            
        results.append(merged)
    
    return pd.concat(results)

def process_large_data_dask(df1_path, df2_path, filter_conditions):
    """
    Process large DataFrames using Dask
    """
    # Read data with Dask
    df1 = dd.read_csv(df1_path)
    df2 = dd.read_csv(df2_path)
    
    # Merge DataFrames
    merged = dd.merge(df1, df2, on='key_column')
    
    # Apply filters
    for condition in filter_conditions:
        merged = merged.query(condition)
    
    return merged.compute()

def process_large_data_vaex(df1_path, df2_path, filter_conditions):
    """
    Process large DataFrames using Vaex
    """
    # Convert CSV to HDF5 format for better performance
    df1_hdf = f"{Path(df1_path).stem}.hdf5"
    df2_hdf = f"{Path(df2_path).stem}.hdf5"
    
    # Read data with Vaex
    df1 = vaex.from_csv(df1_path, convert=True, chunk_size=1_000_000)
    df2 = vaex.from_csv(df2_path, convert=True, chunk_size=1_000_000)
    
    # Merge DataFrames
    merged = df1.join(df2, on='key_column')
    
    # Apply filters
    for condition in filter_conditions:
        merged = merged.filter(condition)
    
    return merged

def process_large_data_arrow(df1_path, df2_path, filter_conditions):
    """
    Process large DataFrames using PyArrow
    """
    # Read data with PyArrow
    table1 = pa.csv.read_csv(df1_path)
    table2 = pa.csv.read_csv(df2_path)
    
    # Convert to pandas for merge operation
    df1 = table1.to_pandas()
    df2 = table2.to_pandas()
    
    # Optimize memory usage
    df1 = optimize_pandas_df(df1)
    df2 = optimize_pandas_df(df2)
    
    # Merge DataFrames
    merged = pd.merge(df1, df2, on='key_column')
    
    # Apply filters
    for condition in filter_conditions:
        merged = merged.query(condition)
    
    return merged

def benchmark_processing_methods(df1_path, df2_path, filter_conditions):
    """
    Benchmark different processing methods
    """
    import time
    results = {}
    
    methods = {
        'pandas_chunked': process_large_data_pandas,
        'dask': process_large_data_dask,
        'vaex': process_large_data_vaex,
        'pyarrow': process_large_data_arrow
    }
    
    for name, method in methods.items():
        start_time = time.time()
        try:
            result = method(df1_path, df2_path, filter_conditions)
            end_time = time.time()
            results[name] = {
                'success': True,
                'time': end_time - start_time,
                'result_size': len(result) if hasattr(result, '__len__') else 'N/A'
            }
        except Exception as e:
            results[name] = {
                'success': False,
                'error': str(e)
            }
    
    return results
